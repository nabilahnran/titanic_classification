{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "copyright"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabilahnran/titanic_classification/blob/main/Titanic_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2hPzRb6j_CA"
      },
      "source": [
        "# Titanic Classification Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDzCkRNv8Kmz"
      },
      "source": [
        "## The Titanic Dataset\n",
        "\n",
        "[Kaggle](https://www.kaggle.com) has a [dataset](https://www.kaggle.com/c/titanic/data) containing the passenger list on the Titanic. The data contains passenger features such as age, gender, ticket class, as well as whether or not they survived.\n",
        "\n",
        "Purpose of the project is to create a binary classifier using TensorFlow and Sklearn to determine if a passenger survived or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4IG3YojoVmk"
      },
      "source": [
        "**Pull the dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeVKtKt9oTmI"
      },
      "source": [
        "! chmod 600 kaggle.json && (ls ~/.kaggle 2>/dev/null || mkdir ~/.kaggle) && cp kaggle.json ~/.kaggle/ && echo 'Done'\n",
        "! kaggle competitions download -c titanic\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nki8KBOgNbId"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('titanic.zip', 'r') as zipObj:\n",
        "   zipObj.extractall('temp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4VxbBaUpnB6"
      },
      "source": [
        "Three files are downloaded:\n",
        "\n",
        "1. `train.csv`: training data (contains features and targets)\n",
        "1. `test.csv`: feature data used to make predictions\n",
        "1. `gender_submission.csv`: an example competition submission file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U_zk4L_HpWJ"
      },
      "source": [
        "## Step 1: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhhX2RJEyADd"
      },
      "source": [
        "**Read Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR0P7qrDx-n9"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('temp/train.csv')\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41IgdcLPv_vn"
      },
      "source": [
        "train_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOz0iyFy1EUN"
      },
      "source": [
        "print(train_data.describe())\n",
        "#pulls out the data with objects dtypes attributes and shows their describe attribute\n",
        "print(train_data.describe(include=['O']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7QL1H7U1duh"
      },
      "source": [
        "test_data = pd.read_csv('temp/test.csv')\n",
        "\n",
        "print(\"train set: \" + str(train_data.shape))\n",
        "print(\"test set: \" + str(test_data.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SCYEBcR2m03"
      },
      "source": [
        "print(train_data.nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa-gbda72um9"
      },
      "source": [
        "for i in train_data.columns:\n",
        "  print(f\"{i}:\", train_data[i].isna().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop useless var and variable which have a many null value (Cabin) and keep fare for cabin alternate**"
      ],
      "metadata": {
        "id": "Jb1Svq7YVteC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANA0C7xk5gy8"
      },
      "source": [
        "#Drop useless var and variable which have a many null value (Cabin)\n",
        "#Keep fare for cabin alternate\n",
        "train_data = train_data.drop(['Name', 'Ticket', 'PassengerId', 'Cabin'], axis=1)\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcF8mxmt_NoE"
      },
      "source": [
        "for i in train_data.columns:\n",
        "  print(\"{}:{}\".format(i, train_data[i].isna().sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyT9JiRuhJ42"
      },
      "source": [
        "train_data.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEpKomg8-MSG"
      },
      "source": [
        "**Processing values for analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxcy47qLChxb"
      },
      "source": [
        "#Not bothering the original data, so copy it\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "Correlation_df = train_data.copy()\n",
        "\n",
        "#Fill NaN cell with the most frequent value on those variable\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "Correlation_df = pd.DataFrame(SimpleImputer(missing_values=np.nan, strategy = 'most_frequent')\n",
        "  .fit_transform(Correlation_df), columns = Correlation_df.columns)\n",
        "\n",
        "#Split to Numeric and Categorical Data for Normalization\n",
        "num_col = train_data.select_dtypes(include = ['float64', 'int64']).columns\n",
        "category_col = train_data.select_dtypes(include = ['object']).columns\n",
        "    \n",
        "#Use OneHotEncoder to categorize Sex and Embarked variable. Also drop the 'embarked_unknown' column after OneHotEncoding\n",
        "Correlation_df_category = pd.get_dummies(Correlation_df[category_col], columns=['Embarked'])\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "Correlation_df_category['Sex'] = LabelEncoder().fit_transform(Correlation_df_category['Sex'])\n",
        "\n",
        "#Normalization num variables\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "Correlation_df_num = pd.DataFrame(StandardScaler().fit(Correlation_df[num_col])\n",
        "  .transform(Correlation_df[num_col]), columns = Correlation_df[num_col].columns)\n",
        "\n",
        "#combine the sub df\n",
        "Correlation_df = pd.concat([Correlation_df_num, Correlation_df_category], axis = 1 )\n",
        "\n",
        "print(Correlation_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Correlation_df[['Sex', 'Embarked_C', 'Embarked_Q', 'Embarked_S']] = Correlation_df[\n",
        "    ['Sex', 'Embarked_C', 'Embarked_Q', 'Embarked_S']].astype('float')\n",
        "Correlation_df"
      ],
      "metadata": {
        "id": "5-WniJpvCZU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC4CDs3_7Yky"
      },
      "source": [
        "for i in Correlation_df.columns:\n",
        "  print(\"{} : {}\".format(i, Correlation_df[i].isna().sum()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measure and visualize correlation between each feature**"
      ],
      "metadata": {
        "id": "7NLBp8vAWm12"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlwS626dOnu7"
      },
      "source": [
        "#Make a correlation table with kendall method\n",
        "Correlation_df.corr(method='kendall')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSblFfRJOwho"
      },
      "source": [
        "#Visualization the Kendall Correlation\n",
        "import seaborn as sns\n",
        "sns.heatmap(Correlation_df.corr(method='kendall'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJZ_3mHUbIjF"
      },
      "source": [
        "#For more obvious correlation comparasion between each column with target column(Survived), I use point biserial correlation\n",
        "from scipy.stats import stats\n",
        "Biserial_df = pd.DataFrame()\n",
        "for i in Correlation_df.drop(columns=['Survived']).columns:\n",
        "  Biserial_df = Biserial_df.append(pd.DataFrame({'Variable': i,\n",
        "                                                 'Correlation': stats.pointbiserialr(Correlation_df[i], Correlation_df['Survived']).correlation,\n",
        "                                                 'P-Value': round(stats.pointbiserialr(Correlation_df[i],Correlation_df['Survived']).pvalue, 3)},\n",
        "                                                index = [0]))\n",
        "  \n",
        "Biserial_df.index = Biserial_df['Variable']\n",
        "Biserial_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-OR0ZMsjTVe"
      },
      "source": [
        "sns.heatmap(Biserial_df[['Correlation']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zQv3rDNnxT6"
      },
      "source": [
        "Conclusion so far:\n",
        "\n",
        "\n",
        "1.   Strongest correlation with Survived variable is Fare, Parch, and Embarked_C.\n",
        "2. Pclass and Fare correlated -0.5 which is worth to anticipate, because pclass and fare should have high correlation.\n",
        "3. Sex has no correlation about surviving/not.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAZlGUQiqIvq"
      },
      "source": [
        "**Data Splitting**\n",
        "\n",
        "Data that will be used is the cleaned data used for correlation before"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = Correlation_df.copy()\n",
        "training_data['Survived'] = training_data['Survived'].astype('int')\n",
        "training_data"
      ],
      "metadata": {
        "id": "KuC1kwwSJysC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = training_data.drop(columns = ['Survived'])\n",
        "train_y = training_data['Survived']\n",
        "\n",
        "#Train&Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_x, validation_x, train_y, validation_y = train_test_split(train_x, train_y, test_size = 0.33, random_state=52)\n",
        "print(train_x.shape, train_y.shape, validation_x.shape, validation_y.shape)"
      ],
      "metadata": {
        "id": "ZxdBxOslvIaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4XKIa8UxAai"
      },
      "source": [
        "train_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAZEuX3fxDq1"
      },
      "source": [
        "train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRiOrGGW6wW6"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxmnIepvmdCx"
      },
      "source": [
        "## Step 2: The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkzDTDMWDjU4"
      },
      "source": [
        "**With Tensorflow**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKgNoBuEm2h0"
      },
      "source": [
        "#---------------Tensorflow----------------\n",
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(32, input_shape = [9], activation = 'relu'),\n",
        "                             tf.keras.layers.Dense(64, activation = 'relu', \n",
        "                                                   kernel_regularizer= tf.keras.regularizers.l1(l=0.1)),\n",
        "                             tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'BinaryCrossentropy', metrics = 'binary_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Xr_nvuFCFC"
      },
      "source": [
        "MLP = model.fit(train_x, train_y, epochs = 1500, validation_data=(validation_x, validation_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbkSrhyZKi52"
      },
      "source": [
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = MLP.history['binary_accuracy']\n",
        "val_acc = MLP.history['val_binary_accuracy']\n",
        "loss = MLP.history['loss']\n",
        "val_loss = MLP.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r')\n",
        "plt.plot(epochs, val_acc, 'b')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r')\n",
        "plt.plot(epochs, val_loss, 'b')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend([\"Loss\", \"Validation Loss\"])\n",
        "\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model with Decision Tree"
      ],
      "metadata": {
        "id": "2pNDE6YHGX7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "DSC = DecisionTreeClassifier()\n",
        "DSC = DSC.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "yc45b6GqI3_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DSC_val_pred = DSC.predict(validation_x)\n",
        "print(\"accuracy : \", metrics.accuracy_score(validation_y, DSC_val_pred))"
      ],
      "metadata": {
        "id": "9P3Pn6K4JZMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "from six import StringIO\n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(DSC, out_file=dot_data,  \n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,feature_names = train_x.columns,class_names=['0','1'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "graph.write_png('titanic_survived.png')\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "pXf5zK4TKr1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryTR9lmuangc"
      },
      "source": [
        "### Model with Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6g4Q-cxaryb"
      },
      "source": [
        "#cari alpha\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "alphas = DecisionTreeClassifier(random_state=0).cost_complexity_pruning_path(train_x, train_y)['ccp_alphas']\n",
        "\n",
        "#Pools of Param\n",
        "random_parameters = {'n_estimators' : [10,100,1000],\n",
        "                     'criterion' : ['gini', 'entropy'],\n",
        "                     'max_depth' : [10,100,1000],\n",
        "                     'max_features' : ['auto', 'sqrt', 'log2'],\n",
        "                     'bootstrap' : [True, False],\n",
        "                     'class_weight': ['balanced', 'balanced_subsample'],\n",
        "                     'ccp_alpha' : alphas}\n",
        "\n",
        "#Customized Cross Validation for Param Tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RFC = RandomizedSearchCV(RandomForestClassifier(),\n",
        "                         param_distributions = random_parameters,\n",
        "                         n_iter = 100,\n",
        "                         scoring = 'accuracy',\n",
        "                         n_jobs = 10,\n",
        "                         cv = 3,\n",
        "                         verbose = 1,\n",
        "                         random_state = 0,\n",
        "                         return_train_score = True)\n",
        "RFC.fit(train_x, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoWdpA5sBROc"
      },
      "source": [
        "Best_param = RFC.best_params_\n",
        "\n",
        "#test with best params\n",
        "RFC = RandomForestClassifier(\n",
        "    n_estimators = Best_param['n_estimators'],\n",
        "    criterion = Best_param['criterion'],\n",
        "    max_depth = Best_param['max_depth'],\n",
        "    max_features = Best_param['max_features'],\n",
        "    bootstrap = Best_param['bootstrap'],\n",
        "    class_weight = Best_param['class_weight'],\n",
        "    ccp_alpha = Best_param['ccp_alpha']\n",
        ")\n",
        "RFC.fit(train_x, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RFC_val_pred= RFC.predict(validation_x)\n",
        "print(\"Accuracy :\", metrics.accuracy_score(validation_y, RFC_val_pred))"
      ],
      "metadata": {
        "id": "AN92Zmn9G17r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "L2fkgasNKS01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "NBC = GaussianNB()\n",
        "NBC = NBC.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "4iJLJe83KOxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NBC_val_pred = NBC.predict(validation_x)\n",
        "print(\"Accuracy : \", metrics.accuracy_score(validation_y, NBC_val_pred))"
      ],
      "metadata": {
        "id": "n9vHe6fZMon1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So, from the trial with Deep Learning-MLP, Decision Tree, Random Forest, and Naive Bayes Classifier, the best one is the Deep Learning-MLP that have highest accuracy: >80%**"
      ],
      "metadata": {
        "id": "3XC0gG0JM-x2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT46j3S26sE2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXJCSsAdz-f0"
      },
      "source": [
        "## Predict with the selected algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEWZUCnT9UkK"
      },
      "source": [
        "test_data = pd.read_csv('temp/test.csv')\n",
        "testing_data = test_data.copy()\n",
        "testing_data.drop(['Name', 'Ticket', 'PassengerId', 'Cabin'], axis = 1, inplace = True)\n",
        "testing_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVtlKU5GgFZL"
      },
      "source": [
        "testing_data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoS5X_EWnFIy"
      },
      "source": [
        "testing_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grnojvNkdOzD"
      },
      "source": [
        "print(test_data)\n",
        "print(testing_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data.dtypes"
      ],
      "metadata": {
        "id": "mpD9YvgdQWVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1p55C9hm_EK"
      },
      "source": [
        "#Split to Numeric and Categorical Data for Normalization\n",
        "num_col = testing_data.select_dtypes(include = ['float64', 'int64']).columns\n",
        "category_col = testing_data.select_dtypes(include = ['object']).columns\n",
        "\n",
        "#fill NaN cell with the most frequent value on those variable\n",
        "testing_data = pd.DataFrame(SimpleImputer(missing_values=np.nan, strategy = 'most_frequent')\n",
        "  .fit_transform(testing_data), columns = testing_data.columns)\n",
        "#Normalization\n",
        "testing_data_num = pd.DataFrame(StandardScaler().fit(testing_data[num_col])\n",
        "  .transform(testing_data[num_col]), columns = testing_data[num_col].columns)\n",
        "print(testing_data)\n",
        "#Use OneHotEncoder to categorize Sex and Embarked variable. Also drop the 'embarked_unknown' column after OneHotEncoding\n",
        "testing_data_category = pd.get_dummies(testing_data[category_col], columns=['Embarked'])\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "testing_data_category['Sex'] = LabelEncoder().fit_transform(testing_data_category['Sex'])\n",
        "\n",
        "\n",
        "\n",
        "#combine the sub df\n",
        "testing_data = pd.concat([testing_data_category, testing_data_num], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_data"
      ],
      "metadata": {
        "id": "PQX5qCtIRfPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ-An_iKrpW6"
      },
      "source": [
        "result = pd.DataFrame()\n",
        "result['Survived'] = model.predict(testing_data).round(0).astype(int).tolist()\n",
        "result['Survived'] = result['Survived'].astype('str').str.extractall('(\\d+)').unstack().fillna('').sum(axis=1).astype('int')\n",
        "\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSw1rDKv6nOO"
      },
      "source": [
        "---"
      ]
    }
  ]
}